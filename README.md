# ImprovingRLHF
Improving the sample and feedback efficiency of reinforcement learning with human feedback.

This code builds upon the code and results achieved by:
Kimin Lee, Laura Smith, Pieter Abbeel, “PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training,” in International Conference on Machine Learning, 2021. 

Reinforcement learning with human feedback allows an agent to complete a hard to define task by using a human participant as a guide. However, current state-of-the-art algorithms lack the efficiency required to deploy these models into real-world scenarios. This is due to the high cost of human feedback and how much feedback would be required using current systems. I present an improvement upon the current systems which allows the agent to have improved sample and feedback efficiency. Specifically, I improve the current methods used to select queries to present to the human by utilising machine learning techniques so that optimal queries are chosen for learning. Additionally, I explore the current methods used for the reward model ensemble and how it can be improved to create a more stable and diverse model. I also explore further methods of improvement that show potential for future updates to reinforcement learning with human feedback algorithms such as new methods of obtaining human feedback and improve the unsupervised pre-training stage. I show that my results are an improvement upon current systems by directly comparing my results against the Feedback-Efficient Interactive Reinforcement Learning via Relabelling Experience and Unsupervised Pre-training (PEBBLE) algorithm. The proposed changes find an improvement in the performance of PEBBLE in terms of the episode reward gained and the rate of growth of the algorithm in more complex environments such as quadruped walk whilst maintaining the high performance achieved in simpler environments like walker. The new algorithm can also improve upon the PEBBLE baseline results even when the amount of human feedback provided is greatly reduced proving that this new method can select better queries for the human to maximise model learning and makes reinforcement learning with human feedback more suitable for real world applications.

The Project_Dissertation contains results and analysis of my results.
